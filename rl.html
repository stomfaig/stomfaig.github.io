<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Gergo Stomfai" />
  <meta name="dcterms.date" content="2025-05-02" />
  <title>My Document Title</title>
  <style>
    html {
      font-size: 12pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">My Document Title</h1>
<p class="author">Gergo Stomfai</p>
<p class="date">2025-05-02</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#short-intro-to-rl" id="toc-short-intro-to-rl">Short intro
to RL</a>
<ul>
<li><a href="#reward-and-return" id="toc-reward-and-return">Reward and
Return</a></li>
<li><a href="#the-goal" id="toc-the-goal">The Goal</a></li>
<li><a href="#a-few-useful-quantities"
id="toc-a-few-useful-quantities">A few useful quantities</a></li>
</ul></li>
<li><a href="#policy-optimization-methods."
id="toc-policy-optimization-methods.">Policy optimization methods.</a>
<ul>
<li><a href="#some-warmup." id="toc-some-warmup.">Some warmup.</a>
<ul>
<li><a href="#exact-policy-iteration."
id="toc-exact-policy-iteration.">Exact policy iteration.</a></li>
<li><a href="#policy-gradient." id="toc-policy-gradient.">Policy
gradient.</a></li>
</ul></li>
<li><a href="#approximate-value-functions-methods."
id="toc-approximate-value-functions-methods.">Approximate value
functions methods.</a>
<ul>
<li><a href="#mixture-policies." id="toc-mixture-policies.">Mixture
policies.</a></li>
<li><a href="#beyond-mixture-policies---trpo"
id="toc-beyond-mixture-policies---trpo">Beyond mixture policies -
TRPO</a></li>
<li><a href="#almost-trpo-ppo" id="toc-almost-trpo-ppo">Almost TRPO:
PPO</a></li>
</ul></li>
<li><a href="#implementations-and-their-consequences."
id="toc-implementations-and-their-consequences.">Implementations and
their consequences.</a>
<ul>
<li><a href="#generalized-advantage-estimation."
id="toc-generalized-advantage-estimation.">Generalized advantage
estimation.</a></li>
<li><a href="#grpo" id="toc-grpo">GRPO</a></li>
</ul></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</nav>
<p><em>A few preliminary remarks.</em> This page is intended to be a
work, where I collect resources, derivations, ideas etc. that I found
useful when diving into the world of RL. In particular, it is not a
hand-held, explain everything type introduction, but rather one that a
person with some mathematical affinity (not much) could understand. As
such, not everything is discussed in detail.</p>
<h1 id="short-intro-to-rl">Short intro to RL</h1>
<p><em>This section is inspired by <a
href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">1</a>.</em></p>
<p><strong>Action space:</strong> The space of actions a model can take
in a given situation.</p>
<p>We can define <strong>policies</strong> given action spaces. A
<strong>policy</strong> is simply a mapping (deterministic or not) from
the current state of the system to a possible action. If a policy is
deterministic, we have: <span class="math display">\[a_t =
\mu(s_t)\]</span> And if it is stochastic: <span class="math display">\[
a_t \sim \mu(\cdot | s_t)\]</span></p>
<p><strong>Remark.</strong> We can think of deterministic policies as
functions mapping from the state to the action. We can think of
non-deterministic policies as classifiers.</p>
<p>The <strong>trajectory</strong> of a system is simply the sequence of
states and actions in the world</p>
<p><span class="math display">\[\tau = (s_0, a_0, s_1,
\ldots)\]</span></p>
<p>The way the next state of the system is determined based on the
current one is governed by the <strong>natural laws of the
environment</strong>: <span class="math display">\[s_{t+1} = f(s_t,
a_t)\]</span> Obviously <span class="math inline">\(f\)</span> can also
be stochastic.</p>
<h3 id="reward-and-return">Reward and Return</h3>
<p>There are several way in which the reward for a given action is
determined:</p>
<p><span class="math display">\[r_t = R(s_t, a_t, s_{t+1})\]</span>
<span class="math display">\[r_t = R(s_t, a_t)\]</span> <span
class="math display">\[r_t = R(s_t)\]</span> The main difference here is
that what actually determines the reward. In the first case, the current
state, the action and the next state all matters. Maybe the next state
doesn’t matter (2nd case). Or maybe the only thing that matters is the
current state. One can easily come up with “real world” examples to all
these.<br />
There are also different ways in which we can define the return of
trajectory: <span class="math display">\[R(\tau) = \sum_{t}^T
r_t\]</span> <span class="math display">\[R(\tau) = \sum_{t} \gamma^t
r_t\]</span> these are for the cases when the time horizon is finite, or
infinite respectively.</p>
<h3 id="the-goal">The Goal</h3>
<p>We are now in position to state the goal of RL. Given a model of the
universe, consider a policy <span class="math inline">\(\pi\)</span> and
the distribution <span class="math inline">\(P(\tau|\pi)\)</span>, which
determines how likely a certain trajectory is given the policy and the
laws of the universe. Then, our aim is to solve the problem</p>
<p><span class="math display">\[\pi^* = \arg \max_\pi E_{\tau \sim
P(\cdot | \pi)} [T(\tau)]\]</span></p>
<p>Where the solution <span class="math inline">\(\pi^*\)</span> is
called the optimal policy.</p>
<h3 id="a-few-useful-quantities">A few useful quantities</h3>
<p>Here we list the definition of a few quantities, that are useful when
comparing policies.</p>
<p><span class="math display">\[\textbf{On-policy value function:} \quad
V_\pi(s) = E_{\tau \sim \pi}[R(\tau) | s_0 = s]\]</span> <span
class="math display">\[\textbf{On-policy action-value function:} \quad
Q_\pi(s, a) = E_{\tau \sim \pi}[R(\tau) | s_0 = s, a_0 = a]\]</span>
<span class="math display">\[\textbf{Optimal value function:} \quad
V_*(s) = \sup_{\pi} E_{\tau \sim \pi}[R(\tau) | s_0 = s]\]</span> <span
class="math display">\[\textbf{Optimal action-value function:} \quad
Q_*(s, a) = \sup_{\pi} E_{\tau \sim \pi}[R(\tau) | s_0 = s, a_0 =
a]\]</span></p>
<p>One can “peel off” the first action in the “value type”- and the
first state change of the world in the “action-value” type equations, to
get a self-consistency condition between these quantities. These are
called the <strong>Bellman equations</strong>.</p>
<p>We can also define the <strong>advantage function</strong> of an
action with respect to a policy <span
class="math inline">\(\pi\)</span>: <span
class="math display">\[A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)\]</span></p>
<h1 id="policy-optimization-methods.">Policy optimization methods.</h1>
<p>The methods we are going to encounter are all based on iteratively
improving a policy that we currently have to a “better one”. To make the
subject of the discussion more clear, we more explicitly clarify the
objectives of the iterative processes we are to constrcuct:</p>
<ol type="1">
<li>We our method to increase some performance measure of the policy
every step.<br />
</li>
<li>We want to be able to verify that our method is improving the
quantity that it is trying to increase.<br />
</li>
<li>We want to be able to investigate the asymptotic behavior of the
method.</li>
</ol>
<h2 id="some-warmup.">Some warmup.</h2>
<h3 id="exact-policy-iteration.">Exact policy iteration.</h3>
<p>The basic idea is the following. If we have access to the exact value
function, given a policy <span class="math inline">\(\pi\)</span>, we
can compute <span class="math inline">\(Q_\pi(s, a)\)</span> explicitly
for each <span class="math inline">\((s,a)\)</span> pair, and create a
new deterministic policy <span class="math inline">\(\pi&#39;\)</span>,
such that <span class="math display">\[\pi&#39;(a;s) = \text{$1$ iff $a
= \arg \max_a Q_\pi(s,a)$, $0$ otherwise}\]</span></p>
<h3 id="policy-gradient.">Policy gradient.</h3>
<p>To take the theory out for a spin, consider the following. Let <span
class="math inline">\(\pi_\theta\)</span> be a parametric family of
policies. Suppose that the initial state of the universe is determined
by some initial distribution <span
class="math inline">\(\rho_0\)</span>. Then, our aim is to find the
policy which would maximize the expression <span
class="math display">\[J(\theta) = E_{\tau \sim \pi_\theta}
[R(\tau)]\]</span> An intuitive way to do this would be to compute the
gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span>,
and perform some form of gradient optimization method.<br />
Let us start by investigating how changing theta affects the evolution
of the universe by thinking about <span
class="math inline">\(\nabla_\theta P(\tau | \theta)\)</span>. By the
<em>“log-derivative trick”</em>: <span
class="math display">\[\nabla_\theta P(\tau | \pi_\theta) = P(\tau |
\pi_\theta) \left( \nabla_\theta \log P(\tau | \pi_{\theta}) \right)
\]</span> The probability of a trajectory <span
class="math inline">\(\tau = (s_0, a_0, \ldots)\)</span> given an
initial distribution <span class="math inline">\(\rho_0\)</span> is
given by: <span class="math display">\[P(\tau | \theta) = \rho(s_0)
\prod_i P(s_{i+1} ; s_i, a_i) \pi_{\theta}(a_i | s_i)\]</span> This way
<span class="math display">\[\log P(\tau | \theta) = \log \rho (s_0) +
\sum_i \log P(s_{i+1}; s_i, a_i) + \log \pi_{\theta}(a_i|s_i)\]</span>
So if now we take the derivative of this with respect to <span
class="math inline">\(\theta\)</span>, all the <span
class="math inline">\(P(s_{i+1}; s_i, a_i)\)</span> terms will vanish,
since they are not dependent on the parameter <span
class="math inline">\(\theta\)</span>. Hence: <span
class="math display">\[\nabla_\theta P(\tau | \theta) = P(\tau | \theta)
\left( \sum_i \nabla_\theta \log \pi_{\theta}(a_i | s_i)
\right)\]</span> So then, <span class="math display">\[\begin{align*}
    \nabla_\theta J(\theta) &amp;= E_{\tau \sim \pi_\theta} [R(\tau)]\\
    &amp;= \nabla_\theta \int_\tau P(\tau | \theta) R(\tau)\\
    &amp;= \int_\tau P(\tau | \theta) \left( \sum_i \nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R(\tau)\\
    &amp;= E_{\tau \sim \pi_\theta} [\left( \sum_i \nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R(\tau)]
\end{align*}\]</span></p>
<h2 id="approximate-value-functions-methods.">Approximate value
functions methods.</h2>
<p>One major drawback of Exact Policy Iteration is the fact that it is
only guaranteed to improve the performance of the policy, if the new
policy has non-negative advantage in every state. This is very hard to
guarantee in practice. Thus, instead of trying to optimize <span
class="math inline">\(\eta(\pi)\)</span> directly, we are going to
define a <em>surrogate</em> objective function. To do this let us fix
some notation first.</p>
<p>Suppose that we have a fixed initial distribution, say <span
class="math inline">\(\mu\)</span>. We will implicitly assume that this
exists, but won’t mention that it exists very often. Then, the
<em>discounted visitation frequencies</em> are given by: <span
class="math display">\[\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) +
\cdots\]</span> And the <em>policy advantage</em> <span
class="math inline">\(\mathbb{A}_{\pi}(\pi&#39;)\)</span> of a policy
<span class="math inline">\(\pi&#39;\)</span> over another policy <span
class="math inline">\(\pi\)</span> as <span
class="math display">\[\mathbb{A}_{\pi}(\pi&#39;) = E_{s \sim d_{\pi,
\mu}} [E_{a \sim \pi&#39;(a;s)} [A_\pi(s, a)]]\]</span> We claim that
the function <span class="math inline">\(L_\pi(\pi_\theta) = J(\pi) +
\mathbb{A}_{\pi} (\pi_\theta)\)</span> (as a function of <span
class="math inline">\(\theta\)</span>) matches <span
class="math inline">\(J(\pi_\theta)\)</span> up to first order. One way
to prove this, is to consider the expansion: <span
class="math display">\[\begin{align*}
    J(\pi_\theta) &amp;= J(\pi) + \sum_t \sum_s P(s_t = s | \pi_\theta)
\sum_a \pi_\theta(a | s) \gamma^t A_\pi(s, a)\\
    &amp;= \cdots\\
    &amp;= J(\pi) + \sum_s \rho_{\pi_\theta}(s) \sum_a \pi_\theta(a|s)
A_\pi(s,a)
\end{align*}\]</span> And then take the derivative and compare it to the
derivative of <span class="math inline">\(L\)</span>.</p>
<p><strong>Exercise.</strong> Fill the gaps in the previous
expansion.</p>
<p><strong>Exercise.</strong> Using the outline above, or in any other
way, prove that <span class="math inline">\(L_\pi(\pi_\theta)\)</span>
agrees with <span class="math inline">\(J(\pi_\theta)\)</span> up to
first order.</p>
<p>There is a natural question to ask now:</p>
<blockquote>
<p>We could simply optimize <span class="math inline">\(J\)</span> by
substituting it with <span class="math inline">\(L\)</span>, and using
some sort of gradient based method. But how long should our step size
be? Is there any way to use the fact that <span
class="math inline">\(L\)</span> and <span
class="math inline">\(J\)</span> are not just arbitrary functions, but
have a fair amount of structure due to the fact that they arise as RL
reward functions?</p>
</blockquote>
<h3 id="mixture-policies.">Mixture policies.</h3>
<p><em>The idea of this section was originally published in <a
href="https://dl.acm.org/doi/10.5555/645531.656005">here</a></em>.</p>
<p>Let’s try to apply the previous idea in the simples setting,
i.e. along a line. <span
class="math display">\[\pi_{\text{new}}^{\alpha}(a;s) = (1 - \alpha)
\pi(a;s) + \alpha \pi&#39;(a; s)\]</span> Where <span
class="math inline">\(0 \leq \alpha \leq 1\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
J(\pi_\text{new}^\alpha) &amp;= L_\pi(\pi&#39;) + O(\alpha^2)\\
&amp;= J(\pi) + \mathbb{A}_{\pi}((1- \alpha) \pi + \alpha \pi&#39;) +
O(\alpha^2)\\
&amp;= J(\pi) + \alpha \mathbb{A}_{\pi}(\pi&#39;) + O(\alpha^2)
\end{align*}\]</span> That is: <span
class="math display">\[\frac{\partial J}{\partial \alpha} \bigg|_{\alpha
= 0} = \frac{1}{1-\gamma} \mathbb{A}_{\pi, \mu}(\pi&#39;)\]</span> Thus
a positive advantage implies the existence of a sufficiently small <span
class="math inline">\(\alpha\)</span> such that the policy <span
class="math inline">\(\pi_{\text{new}}\)</span> is better than <span
class="math inline">\(\pi\)</span>.</p>
<p><strong>Theorem.</strong> Let <span
class="math inline">\(\mathbb{A}\)</span> be the policy advantage of
<span class="math inline">\(\pi&#39;\)</span> with respect to <span
class="math inline">\(\pi\)</span> and the starting distribution <span
class="math inline">\(\mu\)</span>, and let <span
class="math display">\[\varepsilon = \max_s |E_{a \sim \pi&#39;(a,
a)}[A_\pi (s, a)]|\]</span> Then, for every <span
class="math inline">\(\alpha \in [0,1]\)</span>: <span
class="math display">\[\eta_\mu(\pi_{new}) - \eta_{\mu}(\pi) \geq
\frac{\alpha}{1 - \gamma} \left( \mathbb{A} - \frac{2 \alpha \gamma
\varepsilon}{1- \gamma(1 - \alpha)} \right)\]</span></p>
<h3 id="beyond-mixture-policies---trpo">Beyond mixture policies -
TRPO</h3>
<p><em>The idea of this section was originally published in <a
href="https://arxiv.org/abs/1502.05477">here</a></em>.</p>
<p>Fundamentally, we are motivated by the following thought, that
extends the result of the previous section:</p>
<blockquote>
<p>Surely, if there is a new candidate policy <span
class="math inline">\(\pi&#39;\)</span>, such that it has positive
advantage over <span class="math inline">\(\pi\)</span>, and is
sufficiently close (in some sense), then <span
class="math inline">\(\pi&#39;\)</span> must be a better policy than
<span class="math inline">\(\pi\)</span>.</p>
</blockquote>
<p>We are going to introduce two distances. One of them is useful for
proving a bound similar to …, and the other is useful for practical
computations.</p>
<p><strong>Definition.</strong> The <strong>total variation
divergence</strong> between two discrete probability distributions <span
class="math inline">\(p, q\)</span> over the same set is given by: <span
class="math display">\[D_{TV}(p || q) = \frac{1}{2} \sum_i |p_i -
q_i|\]</span> And between policies: <span
class="math display">\[D_{TV}^\text{max}(\pi, \pi&#39;) = \max_s D_{TV}
\big[ \pi(\cdot | s), \pi&#39;(\cdot | s) \big]\]</span></p>
<p>We then have the following theorem, very much in the vein of the
corresponding theorem for Mixture policies:</p>
<p><strong>Theorem.</strong> Let <span class="math inline">\(\alpha =
D_{TV}^{\text{max}}(\pi_{\text{old}}, \pi_{\text{new}})\)</span>. Then:
<span class="math display">\[\eta(\pi_{\text{new}}) \geq
L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4 \varepsilon \gamma}{(1
- \gamma)^2} \alpha^2\]</span></p>
<p>Now, in practice computing the total variation divergence is not
always feasible. It turns out though that it relates to the
KL-divergence in a very useful way: <span
class="math display">\[D_{\text{TV}}(p || q)^2 \leq D_{\text{KL}}(p
|| q)\]</span> And we set <span
class="math display">\[D_{\text{KL}}^{\text{max}} (\pi, \tilde{\pi}) =
\max_s D_{\text{KL}}(\pi(\cdot | s) || \tilde{\pi}(\cdot | s))\]</span>
Which yields the following form of the above theorem: <span
class="math display">\[\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) -
\frac{4 \varepsilon \gamma}{(1 - \gamma)^2} D_{\text{KL}}^{\text{max}}(
\pi, \tilde{\pi})\]</span></p>
<p>An interesting remark to note here.</p>
<blockquote>
<p>We could consider the policy optimization iteration given by the
pseudocode:</p>
<pre><code>while (not converged):  
  pi_new = argmax_pi L_{pi_i}(pi) - CD_KL_max (pi_old, pi)</code></pre>
<p>Which can easily be shown to monotonically improve the policy in
every iteration. The interesting thing is to note the similarity between
this method and <a href="proximal_methods">proximal methods</a> in
optimization theory. It turns out, that if one simply optimizes this
objective function with the constant <span
class="math inline">\(C\)</span> as suggested above, the step sizes of
the method end up being really small. Instead, one can consider a <a
href="trust_region_type_methods">trust-region type method</a>:</p>
<pre><code>while (not converged):  
  pi_new = argmax_pi L_{pi_i}(pi)
      such that CD_KL_max (pi_old, pi) &lt;= delta</code></pre>
</blockquote>
<p>One final thing to note is that one rarely encounters TRPO in the
form we have just described. The following form of the objective is
used:</p>
<h3 id="almost-trpo-ppo">Almost TRPO: PPO</h3>
<h2 id="implementations-and-their-consequences.">Implementations and
their consequences.</h2>
<h3 id="generalized-advantage-estimation.">Generalized advantage
estimation.</h3>
<h3 id="grpo">GRPO</h3>
<h1 id="references">References</h1>
<p><a
href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">OpenAI
Spinning up</a></p>
<p><a href="https://dl.acm.org/doi/10.5555/645531.656005">Sham Kakade
and John Langford. 2002. Approximately Optimal Approximate Reinforcement
Learning. In Proceedings of the Nineteenth International Conference on
Machine Learning (ICML ’02). Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 267–274.</a></p>
<p><a href="https://dl.acm.org/doi/10.5555/645531.656005">Schulman, J.,
Levine, S., Abbeel, P., Jordan, M.I., &amp; Moritz, P. (2015). Trust
Region Policy Optimization. ArXiv, abs/1502.05477.</a></p>
</body>
</html>
