<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Gergo Stomfai" />
  <meta name="dcterms.date" content="2025-05-02" />
  <title>My Document Title</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">My Document Title</h1>
<p class="author">Gergo Stomfai</p>
<p class="date">2025-05-02</p>
</header>
<nav id="TOC" role="doc-toc">

</nav>
<p><em>A few preliminary remarks.</em> This page is intended to be a
work, where I collect resources, derivations, ideas etc. that I found
useful when diving into the world of RL. In particular, it is not a
hand-held, explain everything type introduction, but rather one that a
person with some mathematical affinity (not much) could understand. As
such, not everything is discussed in detail.</p>
<p>One way I believe it is really helpful to think about RL, especially
within the context of this note is the connection between Differential
Geometry and Riemannian Geometry: Fundamentally, ideas often come from
the classical optimization literature, but have a different flavour due
to the highly structured nature of the functions we are trying to
optimize. I will try my best to give references to optimization material
whenever it is appropiate to connect these fields.</p>
<h1 id="short-intro-to-rl">Short intro to RL</h1>
<p><em>This section is inspired by <a
href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">1</a>.</em></p>
<p><strong>Action space:</strong> The space of actions a model can take
in a given situation.</p>
<p>We can define <strong>policies</strong> given action spaces. A
<strong>policy</strong> is simply a mapping (deterministic or not) from
the current state of the system to a possible action. If a policy is
deterministic, we have: <span class="math display">\[a_t =
\mu(s_t)\]</span> And if it is stochastic: <span class="math display">\[
a_t \sim \mu(\cdot | s_t)\]</span></p>
<p><strong>Remark.</strong> We can think of deterministic policies as
functions mapping from the state to the action. We can think of
non-deterministic policies as classifiers.</p>
<p>The <strong>trajectory</strong> of a system is simply the sequence of
states and actions in the world</p>
<p><span class="math display">\[\tau = (s_0, a_0, s_1,
\ldots)\]</span></p>
<p>The way the next state of the system is determined based on the
current one is governed by the <strong>natural laws of the
environment</strong>: <span class="math display">\[s_{t+1} = f(s_t,
a_t)\]</span> Obviously <span class="math inline">\(f\)</span> can also
be stochastic.</p>
<h3 id="reward-and-return">Reward and Return</h3>
<p>There are several way in which the reward for a given action is
determined:</p>
<p><span class="math display">\[r_t = R(s_t, a_t, s_{t+1})\]</span>
<span class="math display">\[r_t = R(s_t, a_t)\]</span> <span
class="math display">\[r_t = R(s_t)\]</span> The main difference here is
that what actually determines the reward. In the first case, the current
state, the action and the next state all matters. Maybe the next state
doesn’t matter (2nd case). Or maybe the only thing that matters is the
current state. One can easily come up with “real world” examples to all
these.<br />
There are also different ways in which we can define the return of
trajectory: <span class="math display">\[R(\tau) = \sum_{t}^T
r_t\]</span> <span class="math display">\[R(\tau) = \sum_{t} \gamma^t
r_t\]</span> these are for the cases when the time horizon is finite, or
infinite respectively.</p>
<h3 id="the-goal">The Goal</h3>
<p>We are now in position to state the goal of RL. Given a model of the
universe, consider a policy <span class="math inline">\(\pi\)</span> and
the distribution <span class="math inline">\(P(\tau|\pi)\)</span>, which
determines how likely a certain trajectory is given the policy and the
laws of the universe. Then, our aim is to solve the problem</p>
<p><span class="math display">\[\pi^* = \arg \max_\pi E_{\tau \sim
P(\cdot | \pi)} [T(\tau)]\]</span></p>
<p>Where the solution <span class="math inline">\(\pi^*\)</span> is
called the optimal policy.</p>
<h3 id="a-few-useful-quantities">A few useful quantities</h3>
<p>Here we list the definition of a few quantities, that are useful when
comparing policies.</p>
<p><span class="math display">\[\textbf{On-policy value function:} \quad
V_\pi(s) = E_{\tau \sim \pi}[R(\tau) | s_0 = s]\]</span> <span
class="math display">\[\textbf{On-policy action-value function:} \quad
Q_\pi(s, a) = E_{\tau \sim \pi}[R(\tau) | s_0 = s, a_0 = a]\]</span>
<span class="math display">\[\textbf{Optimal value function:} \quad
V_*(s) = \sup_{\pi} E_{\tau \sim \pi}[R(\tau) | s_0 = s]\]</span> <span
class="math display">\[\textbf{Optimal action-value function:} \quad
Q_*(s, a) = \sup_{\pi} E_{\tau \sim \pi}[R(\tau) | s_0 = s, a_0 =
a]\]</span></p>
<p>One can “peel off” the first action in the “value type”- and the
first state change of the world in the “action-value” type equations, to
get a self-consistency condition between these quantities. These are
called the <strong>Bellman equations</strong>.</p>
<p>We can also define the <strong>advantage function</strong> of an
action with respect to a policy <span
class="math inline">\(\pi\)</span>: <span
class="math display">\[A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)\]</span></p>
<h3 id="temporal-difference-learning.">Temporal difference
learning.</h3>
<p>In practice the value function is not known, and can be complicated
to compute explicitly. One way though, in which one can try to deduce it
is the following. A self-consistency relation of <span
class="math inline">\(V\)</span> is the following: <span
class="math display">\[V_\pi(s) = E_{a \sim \pi(\cdot | s) } \left[ r(s,
a) +  V(s&#39;) \right]\]</span> Thus given a guess <span
class="math inline">\(V\)</span> for the value function, one use can
tweak <span class="math inline">\(V\)</span> to promote
self-consistency:</p>
<pre><code>Initialize V randomly

while not converged:
    Pick a state s randomly
    Pick an action a according to pi
    V(s) := (1 - a) V(s) + a (r(s, a) + \gamma V(s&#39;))</code></pre>
<p>We achieve perfect self-consistency if the <em>TD-residual of V with
discount <span class="math inline">\(\gamma\)</span></em>, given by
<span class="math display">\[\delta_{a}^{V_{\pi, \gamma}}(s, s&#39;) =
r(s, a) + \gamma V_{\pi}(s&#39;) - V_{\pi}(s)\]</span> satisfies <span
class="math inline">\(E_{a \sim \pi(\cdot|s)} [\delta_{a}^{V^{\pi,
\gamma}}(s, s&#39;)] = 0 \quad (\dagger)\)</span>.</p>
<p>The TD-residual has another application, which we will see in the
section on <a href="#generalized-advantage-estimation">GAE</a>. But to
give you a foretaste, let’s fix <span class="math inline">\(s\)</span>
and <span class="math inline">\(a\)</span> in <span
class="math inline">\((\dagger)\)</span>, and take the expectation with
respect to <span class="math inline">\(s&#39;\)</span>. We get: <span
class="math display">\[\mathbb{E}_{s&#39;}[\delta_a^{V_{\pi, \gamma}}] =
A_{\pi, \gamma}(s, a)\]</span> Without giving too much away, this idea
can be iterated to reduce the variance of this estimation.</p>
<h1 id="policy-optimization-methods.">Policy optimization methods.</h1>
<p>The methods we are going to encounter are all based on iteratively
improving a policy that we currently have to a “better one”. To make the
subject of the discussion more clear, we more explicitly clarify the
objectives of the iterative processes we are to constrcuct:</p>
<ol type="1">
<li>We our method to increase some performance measure of the policy
every step.<br />
</li>
<li>We want to be able to verify that our method is improving the
quantity that it is trying to increase.<br />
</li>
<li>We want to be able to investigate the asymptotic behavior of the
method.</li>
</ol>
<h2 id="some-warmup.">Some warmup.</h2>
<h3 id="exact-policy-iteration.">Exact policy iteration.</h3>
<p>The basic idea is the following. If we have access to the exact value
function, given a policy <span class="math inline">\(\pi\)</span>, we
can compute <span class="math inline">\(Q_\pi(s, a)\)</span> explicitly
for each <span class="math inline">\((s,a)\)</span> pair, and create a
new deterministic policy <span class="math inline">\(\pi&#39;\)</span>,
such that <span class="math display">\[\pi&#39;(a;s) = \text{$1$ iff $a
= \arg \max_a Q_\pi(s,a)$, $0$ otherwise}\]</span></p>
<h3 id="policy-gradient.">Policy gradient.</h3>
<p>To take the theory out for a spin, consider the following. Let <span
class="math inline">\(\pi_\theta\)</span> be a parametric family of
policies. Suppose that the initial state of the universe is determined
by some initial distribution <span
class="math inline">\(\rho_0\)</span>. Then, our aim is to find the
policy which would maximize the expression <span
class="math display">\[J(\theta) = E_{\tau \sim \pi_\theta}
[R(\tau)]\]</span> An intuitive way to do this would be to compute the
gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span>,
and perform some form of gradient optimization method.<br />
Let us start by investigating how changing theta affects the evolution
of the universe by thinking about <span
class="math inline">\(\nabla_\theta P(\tau | \theta)\)</span>. By the
<em>“log-derivative trick”</em>: <span
class="math display">\[\nabla_\theta P(\tau | \pi_\theta) = P(\tau |
\pi_\theta) \left( \nabla_\theta \log P(\tau | \pi_{\theta}) \right)
\]</span> The probability of a trajectory <span
class="math inline">\(\tau = (s_0, a_0, \ldots)\)</span> given an
initial distribution <span class="math inline">\(\rho_0\)</span> is
given by: <span class="math display">\[P(\tau | \theta) = \rho(s_0)
\prod_i P(s_{i+1} ; s_i, a_i) \pi_{\theta}(a_i | s_i)\]</span> This way
<span class="math display">\[\log P(\tau | \theta) = \log \rho (s_0) +
\sum_i \log P(s_{i+1}; s_i, a_i) + \log \pi_{\theta}(a_i|s_i)\]</span>
So if now we take the derivative of this with respect to <span
class="math inline">\(\theta\)</span>, all the <span
class="math inline">\(P(s_{i+1}; s_i, a_i)\)</span> terms will vanish,
since they are not dependent on the parameter <span
class="math inline">\(\theta\)</span>. Hence: <span
class="math display">\[\nabla_\theta P(\tau | \theta) = P(\tau | \theta)
\left( \sum_i \nabla_\theta \log \pi_{\theta}(a_i | s_i)
\right)\]</span> So then, <span class="math display">\[\begin{align*}
    \nabla_\theta J(\theta) &amp;= E_{\tau \sim \pi_\theta} [R(\tau)]\\
    &amp;= \nabla_\theta \int_\tau P(\tau | \theta) R(\tau)\\
    &amp;= \int_\tau P(\tau | \theta) \left( \sum_i \nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R(\tau)\\
    &amp;= E_{\tau \sim \pi_\theta} [\left( \sum_i \nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R(\tau)]
\end{align*}\]</span></p>
<p>Set <span class="math inline">\(R_i(\tau) = \sum_{j \geq t}
r_j\)</span>. Then, <span class="math inline">\(R(\tau) = \sum_{i &lt;
t} r_i + R_t(\tau)\)</span>, and consequentially: <span
class="math display">\[\begin{align*}
    E_{\tau \sim \pi_\theta} [\left( \sum_i \nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R(\tau)] &amp;= \sum_i E_{\tau \sim
\pi_\theta}[\left(\nabla_\theta \log \pi_{\theta}(a_i | s_i) \right)
R(\tau)]\\
    &amp;= \sum_i E_{\tau \sim \pi_\theta}[\left(\nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) (R_i + \sum_{j&lt;t} r_j)]\\
    &amp;= \sum_i E_{\tau \sim \pi_\theta}[\left(\nabla_\theta \log
\pi_{\theta}(a_i | s_i) \right) R_i]
\end{align*}\]</span></p>
<p>Where the last inequality follows from the fact that the action <span
class="math inline">\(a_t\)</span> is independent of the states <span
class="math inline">\(s_i\)</span> for <span class="math inline">\(i
&lt; t\)</span>. This gives the <strong>past independent form</strong>
of the policy gradient: <span
class="math display">\[\boxed{\nabla_\theta J(\theta) = E_{\tau \sim
\pi_\theta} [\left( \sum_i \nabla_\theta \log \pi_{\theta}(a_i | s_i)
\right) R_i(\tau)]}\]</span></p>
<p>Another useful trick is to notice that for <span
class="math inline">\(s\)</span> fixed, <span
class="math inline">\(\pi_\theta(\cdot|s)\)</span> becomes a
parameterized distribution on <span class="math inline">\(A\)</span>. It
is a general result, that <span class="math inline">\(E_{a \sim
\pi_{(\cdot|s)}} [\nabla_\theta \log \pi_\theta(a|s)] = 0\)</span>. Upon
multiplying this with a constant <span
class="math inline">\(b(s_t)\)</span>, we obtain that <span
class="math display">\[E_{a \sim \pi_{(\cdot|s)}} [\nabla_\theta \log
\pi_\theta(a|s) b(s)] = 0\]</span> We can also write <span
class="math display">\[\begin{align*}
E_{\tau \sim \pi_{\theta}}[\nabla_\theta \log \pi_\theta(a_i|s_i)
b(s_i)] &amp;= \sum_{s, a} P(s_i = a, a_i = a) \nabla_\theta \log
\pi_\theta(a_i|s_i) b(s_i)\\
&amp;= \sum_s P(s_i = s) \left[ \sum_a P(a_i = a | s_i = s)
\nabla_\theta \log \pi_\theta(a_i|s_i) b(s_i) \right]\\
&amp;= \sum_s P(s_i = s) \left[ E_{a \sim \pi_\theta(\cdot|s)}
[\nabla_\theta \log \pi_\theta(a|s) b(s_i)] \right] = 0
\end{align*}\]</span></p>
<p>Using this claim, we can derive another unbiased estimator for the
policy gradient:</p>
<p><span class="math display">\[\boxed{\nabla_\theta J(\theta) = E_{\tau
\sim \pi_\theta} [\left( \sum_i \nabla_\theta \log \pi_{\theta}(a_i |
s_i) \right) \left( R_i(\tau) + b(s_t) \right)]} \]</span></p>
<p>The main difference between these is that some of them have lower
variance than the other. While it is not instrumental to know all these
derivations inside out, it is useful to be aware of the different forms
of the policy gradient.</p>
<h2 id="approximate-value-functions-methods.">Approximate value
functions methods.</h2>
<p>One major drawback of Exact Policy Iteration is the fact that it is
only guaranteed to improve the performance of the policy, if the new
policy has non-negative advantage in every state. This is very hard to
guarantee in practice. Thus, instead of trying to optimize <span
class="math inline">\(\eta(\pi)\)</span> directly, we are going to
define a <em>surrogate</em> objective function. To do this let us fix
some notation first.</p>
<p>Suppose that we have a fixed initial distribution, say <span
class="math inline">\(\mu\)</span>. We will implicitly assume that this
exists, but won’t mention that it exists very often. Then, the
<em>discounted visitation frequencies</em> are given by: <span
class="math display">\[\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) +
\cdots\]</span> And the <em>policy advantage</em> <span
class="math inline">\(\mathbb{A}_{\pi}(\pi&#39;)\)</span> of a policy
<span class="math inline">\(\pi&#39;\)</span> over another policy <span
class="math inline">\(\pi\)</span> as <span
class="math display">\[\mathbb{A}_{\pi}(\pi&#39;) = E_{s \sim d_{\pi,
\mu}} [E_{a \sim \pi&#39;(a;s)} [A_\pi(s, a)]]\]</span> We claim that
the function <span class="math inline">\(L_\pi(\pi_\theta) = J(\pi) +
\mathbb{A}_{\pi} (\pi_\theta)\)</span> (as a function of <span
class="math inline">\(\theta\)</span>) matches <span
class="math inline">\(J(\pi_\theta)\)</span> up to first order. One way
to prove this, is to consider the expansion: <span
class="math display">\[\begin{align*}
    J(\pi_\theta) &amp;= J(\pi) + \sum_t \sum_s P(s_t = s | \pi_\theta)
\sum_a \pi_\theta(a | s) \gamma^t A_\pi(s, a)\\
    &amp;= \cdots\\
    &amp;= J(\pi) + \sum_s \rho_{\pi_\theta}(s) \sum_a \pi_\theta(a|s)
A_\pi(s,a)
\end{align*}\]</span> And then take the derivative and compare it to the
derivative of <span class="math inline">\(L\)</span>.</p>
<p><strong>Exercise.</strong> Fill the gaps in the previous
expansion.</p>
<p><strong>Exercise.</strong> Using the outline above, or in any other
way, prove that <span class="math inline">\(L_\pi(\pi_\theta)\)</span>
agrees with <span class="math inline">\(J(\pi_\theta)\)</span> up to
first order.</p>
<p>There is a natural question to ask now:</p>
<blockquote>
<p>We could simply optimize <span class="math inline">\(J\)</span> by
substituting it with <span class="math inline">\(L\)</span>, and using
some sort of gradient based method. But how long should our step size
be? Is there any way to use the fact that <span
class="math inline">\(L\)</span> and <span
class="math inline">\(J\)</span> are not just arbitrary functions, but
have a fair amount of structure due to the fact that they arise as RL
reward functions?</p>
</blockquote>
<h3 id="mixture-policies.">Mixture policies.</h3>
<p><em>The idea of this section was originally published in <a
href="https://dl.acm.org/doi/10.5555/645531.656005">here</a></em>.</p>
<p>Let’s try to apply the previous idea in the simples setting,
i.e. along a line. <span
class="math display">\[\pi_{\text{new}}^{\alpha}(a;s) = (1 - \alpha)
\pi(a;s) + \alpha \pi&#39;(a; s)\]</span> Where <span
class="math inline">\(0 \leq \alpha \leq 1\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
J(\pi_\text{new}^\alpha) &amp;= L_\pi(\pi&#39;) + O(\alpha^2)\\
&amp;= J(\pi) + \mathbb{A}_{\pi}((1- \alpha) \pi + \alpha \pi&#39;) +
O(\alpha^2)\\
&amp;= J(\pi) + \alpha \mathbb{A}_{\pi}(\pi&#39;) + O(\alpha^2)
\end{align*}\]</span> That is: <span
class="math display">\[\frac{\partial J}{\partial \alpha} \bigg|_{\alpha
= 0} = \frac{1}{1-\gamma} \mathbb{A}_{\pi, \mu}(\pi&#39;)\]</span> Thus
a positive advantage implies the existence of a sufficiently small <span
class="math inline">\(\alpha\)</span> such that the policy <span
class="math inline">\(\pi_{\text{new}}\)</span> is better than <span
class="math inline">\(\pi\)</span>.</p>
<p><strong>Theorem.</strong> Let <span
class="math inline">\(\mathbb{A}\)</span> be the policy advantage of
<span class="math inline">\(\pi&#39;\)</span> with respect to <span
class="math inline">\(\pi\)</span> and the starting distribution <span
class="math inline">\(\mu\)</span>, and let <span
class="math display">\[\varepsilon = \max_s |E_{a \sim \pi&#39;(a,
a)}[A_\pi (s, a)]|\]</span> Then, for every <span
class="math inline">\(\alpha \in [0,1]\)</span>: <span
class="math display">\[\eta_\mu(\pi_{new}) - \eta_{\mu}(\pi) \geq
\frac{\alpha}{1 - \gamma} \left( \mathbb{A} - \frac{2 \alpha \gamma
\varepsilon}{1- \gamma(1 - \alpha)} \right)\]</span></p>
<h3 id="beyond-mixture-policies---trpo">Beyond mixture policies -
TRPO</h3>
<p><em>The idea of this section was originally published in
[here][3]</em>.</p>
<p>Fundamentally, we are motivated by the following thought, that
extends the result of the previous section:</p>
<blockquote>
<p>Surely, if there is a new candidate policy <span
class="math inline">\(\pi&#39;\)</span>, such that it has positive
advantage over <span class="math inline">\(\pi\)</span>, and is
sufficiently close (in some sense), then <span
class="math inline">\(\pi&#39;\)</span> must be a better policy than
<span class="math inline">\(\pi\)</span>.</p>
</blockquote>
<p>We are going to introduce two distances. One of them is useful for
proving a bound similar to …, and the other is useful for practical
computations.</p>
<p><strong>Definition.</strong> The <strong>total variation
divergence</strong> between two discrete probability distributions <span
class="math inline">\(p, q\)</span> over the same set is given by: <span
class="math display">\[D_{TV}(p || q) = \frac{1}{2} \sum_i |p_i -
q_i|\]</span> And between policies: <span
class="math display">\[D_{TV}^\text{max}(\pi, \pi&#39;) = \max_s D_{TV}
\big[ \pi(\cdot | s), \pi&#39;(\cdot | s) \big]\]</span></p>
<p>We then have the following theorem, very much in the vein of the
corresponding theorem for Mixture policies:</p>
<p><strong>Theorem.</strong> Let <span class="math inline">\(\alpha =
D_{TV}^{\text{max}}(\pi_{\text{old}}, \pi_{\text{new}})\)</span>. Then:
<span class="math display">\[\eta(\pi_{\text{new}}) \geq
L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4 \varepsilon \gamma}{(1
- \gamma)^2} \alpha^2\]</span></p>
<p>Now, in practice computing the total variation divergence is not
always feasible. It turns out though that it relates to the
KL-divergence in a very useful way: <span
class="math display">\[D_{\text{TV}}(p || q)^2 \leq D_{\text{KL}}(p
|| q)\]</span> And we set <span
class="math display">\[D_{\text{KL}}^{\text{max}} (\pi, \tilde{\pi}) =
\max_s D_{\text{KL}}(\pi(\cdot | s) || \tilde{\pi}(\cdot | s))\]</span>
Which yields the following form of the above theorem: <span
class="math display">\[\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) -
\frac{4 \varepsilon \gamma}{(1 - \gamma)^2} D_{\text{KL}}^{\text{max}}(
\pi, \tilde{\pi})\]</span></p>
<p>An interesting remark to note here.</p>
<blockquote>
<p>We could consider the policy optimization iteration given by the
pseudocode:</p>
<pre><code>while (not converged):  
  pi_new = argmax_pi L_{pi_i}(pi) - CD_KL_max (pi_old, pi)</code></pre>
<p>Which can easily be shown to monotonically improve the policy in
every iteration. The interesting thing is to note the similarity between
this method and <a href="proximal_methods">proximal methods</a> in
optimization theory. It turns out, that if one simply optimizes this
objective function with the constant <span
class="math inline">\(C\)</span> as suggested above, the step sizes of
the method end up being really small. Instead, one can consider a <a
href="trust_region_type_methods">trust-region type method</a>:</p>
<pre><code>while (not converged):  
  pi_new = argmax_pi L_{pi_i}(pi)
      such that CD_KL_max (pi_old, pi) &lt;= delta</code></pre>
</blockquote>
<p><strong>Implementation.</strong> We have enough machinery now to
think about how we might want to implement this method. We need to come
up with a way to approxiamte <span
class="math display">\[L_{\theta_{\text{old}}}(\theta) = \sum_s
\rho_{\text{old}}(s) \sum_a \pi_\theta(a | s)
A_{\theta_{\text{old}}}(s,a)\]</span></p>
<p>The first and most obvious thing that needs to happen, is
approximating <span class="math inline">\(\sum_s
\rho_{\text{old}}(s)[\cdots]\)</span> with a montecarlo estiamte based
on our data.</p>
<p>Then, we need a way to estimate the advantage <span
class="math inline">\(A_{\theta_{\text{old}}}\)</span>. Though there are
several ways to do this, for now we are simply going to stick to a
simply approximation: <span
class="math display">\[\hat{A}_{\theta_{\text{old}}} =
Q_{\theta_{\text{old}}}\]</span> Which is <strong>only true up to an
additive constant</strong>, but that is good enough for our purposes.
For another method see the section on <a
href="#generalized-advantage-estimation">GAE</a>.</p>
<p>Finally, we replace the sum over actions with an importance
sampler.</p>
<p>These three together then given the empirically computable version of
<span class="math inline">\(L\)</span>: <span
class="math display">\[L_{\theta_{\text{old}}} = \mathbb{E}_{s \sim
\rho_{\text{old}}, a \sim q} \left[ \frac{\pi_\theta(a|s)}{q(a|s)}
Q_{\theta_{\text{old}}}(s,a) \right]\]</span></p>
<p><strong>Some practical considerations.</strong> It is now a fitting
time to think about implementation.</p>
<p>Another way of approximating <span
class="math inline">\(L_{\theta_{\text{old}}}\)</span> can be found in
the article <a href="asd">asd</a>.</p>
<h3 id="almost-trpo-ppo">Almost TRPO: PPO</h3>
<p><em>See</em></p>
<p>The conclusion of the path-sampling strategy in the <a
href="#beyond-mixture-policies---trpo">previous section</a> was
optimizing the objective (we omit <span
class="math inline">\(\theta_{\text{old}}\)</span> since it is clear
from the notation what we mean) <span class="math display">\[L(\theta) =
\mathbb{E} \left[ \frac{\pi_\theta(a_t |
s_t)}{\pi_{\theta_{\text{old}}}(a_t| s_t)} \hat{A}_t \right]\]</span>
For ease of notation, we denote the quantity <span
class="math inline">\(\frac{\pi_\theta(a_t |
s_t)}{\pi_{\theta_{\text{old}}}(a_t| s_t)}\)</span> by <span
class="math inline">\(r_t(\theta)\)</span>. Intuitively, if <span
class="math inline">\(r_t(\theta)\)</span> is far from <span
class="math inline">\(1\)</span>, the updates can be jerky and large.
This is undesirable for many reasons, which we won’t go into in too much
detail, but here is an illustrative tale from the world of
optimization.</p>
<blockquote>
<p>tale from opti</p>
</blockquote>
<p>One way to ensure this, is to disincentivise the model from making
large changes in the policy. As you can (and should) check, the
following objective does exactly that:</p>
<p><span class="math display">\[L(\theta) = \mathbb{E} \left\{ \min
\left[ r_t(\theta) A_t, \operatorname{clip}(r_t(\theta, 1- \epsilon, 1 +
\epsilon)) \right] \right\}\]</span></p>
<h3 id="generalized-advantage-estimation.">Generalized advantage
estimation.</h3>
<p><em>This section is based on <a
href="https://arxiv.org/abs/1506.02438">Schulman et.
al. 2015</a>.</em></p>
<p>We have already seen the need for a way to approximate the advantage
<span class="math inline">\(A_t(a)\)</span> if an action compared to
some reference policy. In the case of TRPO we chose one of the easiest
routes. We now describe a more elaborate one.</p>
<p>As promised in the section on <a
href="#temporal-difference-learning">Temporal Difference Learning</a>,
the core comes from the fact that <span
class="math inline">\(\mathbb{E}_{s&#39;}[\delta_a^{V_{\pi, \gamma}}] =
A_{\pi, \gamma}(s, a)\)</span>. In fact, we consider the estimates <span
class="math inline">\(\hat{A}_t^{(i)}\)</span> given by:</p>
<h3 id="grpo">GRPO</h3>
<p><em>This section is based on [asd]</em>.</p>
<p>The fundamental idea here can be explained as follows.</p>
<blockquote>
<p>In PPO (and TRPO for that regard), we need a way to approximate the
advantage <span class="math inline">\(A_\pi(s, a)\)</span>. We could
simply use <span class="math inline">\(Q_\pi(s,a)\)</span>, but this is
usually has high variance. We could instead use GAE, but then one also
needs to train a value model <span class="math inline">\(V\)</span>,
which can be similar in size and complexity to the main policy
model.</p>
</blockquote>
<hr />
<p><em>References.</em></p>
<p><a
href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">OpenAI
Spinning up</a></p>
<p><a href="https://dl.acm.org/doi/10.5555/645531.656005">Sham Kakade
and John Langford. 2002. Approximately Optimal Approximate Reinforcement
Learning. In Proceedings of the Nineteenth International Conference on
Machine Learning (ICML ’02). Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 267–274.</a></p>
<p><a href="https://arxiv.org/abs/1502.05477">Schulman, J., Levine, S.,
Abbeel, P., Jordan, M.I., &amp; Moritz, P. (2015). Trust Region Policy
Optimization. ArXiv, abs/1502.05477.</a></p>
<p><a href="https://arxiv.org/abs/1506.02438">Schulman, J., Moritz, P.,
Levine, S., Jordan, M., &amp; Abbeel, P. (2015). High-dimensional
continuous control using generalized advantage estimation. arXiv
preprint arXiv:1506.02438.</a></p>
</body>
</html>
